{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir_loc = 'fgo_image_datasets'\n",
    "task_names = ['turncounter','cards','attack']\n",
    "\n",
    "task_node_dict = {'attack':['attack','not_attack'],\n",
    "                  'cards':['quick','arts','buster'],\n",
    "                  'turncounter':['one','two','three']}\n",
    "\n",
    "task_list = listdir(base_dir_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attack', 'cards', 'turncounter']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_ds_store(base_list):\n",
    "    if '.DS_Store' in base_list:\n",
    "        base_list.remove('.DS_Store')\n",
    "    return base_list\n",
    "\n",
    "remove_ds_store(task_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 71\n",
      "159 159\n",
      "52 52\n",
      "52 52\n",
      "49 49\n",
      "74 74\n",
      "72 72\n",
      "60 60\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "for task in task_list:\n",
    "    task_file_list = []\n",
    "    task_label_list = []\n",
    "    \n",
    "    individual_task_labels = task_node_dict[task]\n",
    "    for individual_task in individual_task_labels:\n",
    "        task_path = 'fgo_image_datasets'+'/'+task+'/'+individual_task\n",
    "        individual_task_list = listdir(task_path)\n",
    "        task_label_name = [individual_task]*len(individual_task_list)\n",
    "        \n",
    "        individual_task_full_path = []\n",
    "        for individual_file in individual_task_list:\n",
    "            individual_loc = task_path+'/'+individual_file\n",
    "            individual_task_full_path.append(individual_loc)\n",
    "\n",
    "        task_file_list+= individual_task_full_path\n",
    "        task_label_list += task_label_name\n",
    "        print(len(individual_task_list),len(task_label_name))\n",
    "    dat = pd.DataFrame({'file_path':task_file_list,'label':task_label_list})\n",
    "    dat.to_csv(task+'.csv',index=False)\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_dat = pd.read_csv('attack.csv')\n",
    "cards_dat = pd.read_csv('cards.csv')\n",
    "turncounter_dat = pd.read_csv('turncounter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "      <th>label_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>fgo_image_datasets/attack/attack/a1.JPG</td>\n",
       "      <td>attack</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>fgo_image_datasets/attack/attack/a10.JPG</td>\n",
       "      <td>attack</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>fgo_image_datasets/attack/attack/a11.JPG</td>\n",
       "      <td>attack</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>fgo_image_datasets/attack/attack/a12.JPG</td>\n",
       "      <td>attack</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>fgo_image_datasets/attack/attack/a13.JPG</td>\n",
       "      <td>attack</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  file_path   label  label_cat\n",
       "0   fgo_image_datasets/attack/attack/a1.JPG  attack          0\n",
       "1  fgo_image_datasets/attack/attack/a10.JPG  attack          0\n",
       "2  fgo_image_datasets/attack/attack/a11.JPG  attack          0\n",
       "3  fgo_image_datasets/attack/attack/a12.JPG  attack          0\n",
       "4  fgo_image_datasets/attack/attack/a13.JPG  attack          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_dat[\"label\"] = attack_dat[\"label\"].astype('category')\n",
    "attack_dat[\"label_cat\"] = attack_dat[\"label\"].cat.codes\n",
    "\n",
    "attack_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "      <th>label_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>fgo_image_datasets/cards/quick/Capture.JPG</td>\n",
       "      <td>quick</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>fgo_image_datasets/cards/quick/q10.JPG</td>\n",
       "      <td>quick</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>fgo_image_datasets/cards/quick/q11.JPG</td>\n",
       "      <td>quick</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>fgo_image_datasets/cards/quick/q12.JPG</td>\n",
       "      <td>quick</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>fgo_image_datasets/cards/quick/q13.JPG</td>\n",
       "      <td>quick</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    file_path  label  label_cat\n",
       "0  fgo_image_datasets/cards/quick/Capture.JPG  quick          2\n",
       "1      fgo_image_datasets/cards/quick/q10.JPG  quick          2\n",
       "2      fgo_image_datasets/cards/quick/q11.JPG  quick          2\n",
       "3      fgo_image_datasets/cards/quick/q12.JPG  quick          2\n",
       "4      fgo_image_datasets/cards/quick/q13.JPG  quick          2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards_dat[\"label\"] = cards_dat[\"label\"].astype('category')\n",
    "cards_dat[\"label_cat\"] = cards_dat[\"label\"].cat.codes\n",
    "\n",
    "cards_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "      <th>label_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>fgo_image_datasets/turncounter/one/1.JPG</td>\n",
       "      <td>one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>fgo_image_datasets/turncounter/one/10.JPG</td>\n",
       "      <td>one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>fgo_image_datasets/turncounter/one/11.JPG</td>\n",
       "      <td>one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>fgo_image_datasets/turncounter/one/12.JPG</td>\n",
       "      <td>one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>fgo_image_datasets/turncounter/one/13.JPG</td>\n",
       "      <td>one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   file_path label  label_cat\n",
       "0   fgo_image_datasets/turncounter/one/1.JPG   one          0\n",
       "1  fgo_image_datasets/turncounter/one/10.JPG   one          0\n",
       "2  fgo_image_datasets/turncounter/one/11.JPG   one          0\n",
       "3  fgo_image_datasets/turncounter/one/12.JPG   one          0\n",
       "4  fgo_image_datasets/turncounter/one/13.JPG   one          0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turncounter_dat[\"label\"] = turncounter_dat[\"label\"].astype('category')\n",
    "turncounter_dat[\"label_cat\"] = turncounter_dat[\"label\"].cat.codes\n",
    "\n",
    "turncounter_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    74\n",
       "2    72\n",
       "1    60\n",
       "Name: label_cat, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turncounter_dat[\"label_cat\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211, 3) (19, 3)\n",
      "(142, 3) (11, 3)\n",
      "(186, 3) (20, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_test_split(df,train_percent):\n",
    "    msk = np.random.rand(len(df)) < train_percent\n",
    "    train = df[msk]\n",
    "    test = df[~msk]\n",
    "    \n",
    "    return train,test\n",
    "\n",
    "train_attack,test_attack = train_test_split(attack_dat,.9)\n",
    "print(train_attack.shape,test_attack.shape)\n",
    "\n",
    "train_cards,test_cards = train_test_split(cards_dat,.9)\n",
    "print(train_cards.shape,test_cards.shape)\n",
    "\n",
    "train_turncounter,test_turncounter = train_test_split(turncounter_dat,.9)\n",
    "print(train_turncounter.shape,test_turncounter.shape)\n",
    "\n",
    "train_attack = train_attack.reset_index()\n",
    "test_attack = test_attack.reset_index()\n",
    "train_cards = train_cards.reset_index()\n",
    "test_cards = test_cards.reset_index()\n",
    "train_turncounter = train_turncounter.reset_index()\n",
    "test_turncounter = test_turncounter.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tonks stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0112 22:47:48.034691  3368 file_utils.py:35] PyTorch version 1.2.0 available.\n"
     ]
    }
   ],
   "source": [
    "from tonks import MultiTaskLearner, MultiDatasetLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "imagenet_rgb_means = [0.485, 0.456, 0.406]\n",
    "imagenet_rgb_std = [0.229, 0.224, 0.225]\n",
    "resnet_img_size = 224\n",
    "\n",
    "full_img_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomResizedCrop(resnet_img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(imagenet_rgb_means, imagenet_rgb_std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((resnet_img_size, resnet_img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(imagenet_rgb_means, imagenet_rgb_std)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "class FGOImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Load data specifically for use with a image models\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: pandas Series\n",
    "        file paths to stored images\n",
    "    y: list\n",
    "        A list of dummy-encoded categories\n",
    "        For instance, y might be [0,1,2,0] for a 3 class problem with 4 samples\n",
    "    transform: str or list of pytorch transforms\n",
    "        specifies how to preprocess the full image for an attribute image model\n",
    "        To use built in tonks image transforms use strings `train` or `val`\n",
    "        To use custom transformations supply a list of pytorch transforms.\n",
    "    crop_transform: str or list of pytorch transforms\n",
    "        specifies how to preprocess the center cropped image for an attribute image model\n",
    "        To use built in tonks image transforms use strings `train` or `val`\n",
    "        To use custom transformations supply a list of pytorch transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 x,\n",
    "                 y,\n",
    "                 transform='train'):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "        if transform == 'train' or 'val':\n",
    "            self.transform = full_img_transforms[transform]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return tuple of images as pytorch tensors and and tensor of labels\"\"\"\n",
    "        label = self.y[index]\n",
    "        full_img = Image.open(self.x[index]).convert('RGB')\n",
    "\n",
    "        full_img = self.transform(full_img)\n",
    "\n",
    "        label = torch.from_numpy(np.array(label)).float()\n",
    "\n",
    "        return full_img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "train_attack,test_attack = train_test_split(attack_dat,.9)\n",
    "print(train_attack.shape,test_attack.shape)\n",
    "\n",
    "train_cards,test_cards = train_test_split(cards_dat,.9)\n",
    "print(train_cards.shape,test_cards.shape)\n",
    "\n",
    "train_turncounter,test_turncounter = train_test_split(turncounter_dat,.9)\n",
    "print(train_turncounter.shape,test_turncounter.shape)\n",
    "\n",
    "'''\n",
    "attack_train_dataset = FGOImageDataset(\n",
    "    x=train_attack['file_path'],\n",
    "    y=train_attack['label_cat'],\n",
    "    transform='train')\n",
    "attack_test_dataset = FGOImageDataset(\n",
    "    x=test_attack['file_path'],\n",
    "    y=test_attack['label_cat'],\n",
    "    transform='val')\n",
    "\n",
    "\n",
    "cards_train_dataset = FGOImageDataset(\n",
    "    x=train_cards['file_path'],\n",
    "    y=train_cards['label_cat'],\n",
    "    transform='train')\n",
    "cards_test_dataset = FGOImageDataset(\n",
    "    x=test_cards['file_path'],\n",
    "    y=test_cards['label_cat'],\n",
    "    transform='val')\n",
    "\n",
    "\n",
    "turncounter_train_dataset = FGOImageDataset(\n",
    "    x=train_turncounter['file_path'],\n",
    "    y=train_turncounter['label_cat'],\n",
    "    transform='train')\n",
    "turncounter_test_dataset = FGOImageDataset(\n",
    "    x=test_turncounter['file_path'],\n",
    "    y=test_turncounter['label_cat'],\n",
    "    transform='val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dataloaders_dict = {\n",
    "    'attack': DataLoader(attack_train_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'cards': DataLoader(cards_train_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'turncounter': DataLoader(turncounter_train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}\n",
    "\n",
    "valid_dataloaders_dict = {\n",
    "   'attack': DataLoader(attack_test_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'cards': DataLoader(cards_test_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'turncounter': DataLoader(turncounter_test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TrainLoader = MultiDatasetLoader(loader_dict=train_dataloaders_dict)\n",
    "print(len(TrainLoader))\n",
    "\n",
    "ValidLoader = MultiDatasetLoader(loader_dict=valid_dataloaders_dict, shuffle=False)\n",
    "print(len(ValidLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_task_dict = {\n",
    "    'attack': attack_dat['label_cat'].nunique(),\n",
    "    'cards': cards_dat['label_cat'].nunique(),\n",
    "    'turncounter': turncounter_dat['label_cat'].nunique(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attack': 2, 'cards': 3, 'turncounter': 3}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_task_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models as torch_models\n",
    "\n",
    "from tonks.vision.helpers import _dense_block, _Identity\n",
    "\n",
    "\n",
    "class ResnetForMultiTaskClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch image attribute model. This model allows you to load\n",
    "    in some pretrained tasks in addition to creating new ones.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    To instantiate a completely new instance of ResnetForMultiTaskClassification\n",
    "    and load the weights into this architecture you can set `pretrained` to True\n",
    "\n",
    "    ```\n",
    "        model = ResnetForMultiTaskClassification(\n",
    "            new_task_dict=new_task_dict,\n",
    "            load_pretrained_resnet = True\n",
    "        )\n",
    "\n",
    "        DO SOME TRAINING\n",
    "\n",
    "        model.save(SOME_FOLDER, SOME_MODEL_ID)\n",
    "    ```\n",
    "\n",
    "    To instantiate an instance of ResnetForMultiTaskClassification that has layers for\n",
    "    pretrained tasks and new tasks, you would do the following:\n",
    "    ```\n",
    "        model = ResnetForMultiTaskClassification(\n",
    "            pretrained_task_dict=pretrained_task_dict,\n",
    "            new_task_dict=new_task_dict\n",
    "        )\n",
    "\n",
    "        model.load(SOME_FOLDER, SOME_MODEL_DICT)\n",
    "\n",
    "        DO SOME TRAINING\n",
    "    ```\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pretrained_task_dict: dict\n",
    "        dictionary mapping each pretrained task to the number of labels it has\n",
    "    new_task_dict: dict\n",
    "        dictionary mapping each new task to the number of labels it has\n",
    "    load_pretrained_resnet: boolean\n",
    "        flag for whether or not to load in pretrained weights for ResNet50.\n",
    "        useful for the first round of training before there are fine tuned weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_task_dict=None, new_task_dict=None, load_pretrained_resnet=False):\n",
    "        super(ResnetForMultiTaskClassification, self).__init__()\n",
    "\n",
    "        self.resnet = torch_models.resnet50(pretrained=load_pretrained_resnet)\n",
    "        self.resnet.fc = _Identity()\n",
    "\n",
    "        if pretrained_task_dict is not None:\n",
    "            pretrained_layers = {}\n",
    "            for key, task_size in pretrained_task_dict.items():\n",
    "                pretrained_layers[key] = nn.Linear(2048, task_size)\n",
    "            self.pretrained_classifiers = nn.ModuleDict(pretrained_layers)\n",
    "        if new_task_dict is not None:\n",
    "            new_layers = {}\n",
    "            for key, task_size in new_task_dict.items():\n",
    "                new_layers[key] = nn.Linear(2048, task_size)\n",
    "            self.new_classifiers = nn.ModuleDict(new_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines forward pass for image model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: dict of image tensors containing tensors for\n",
    "        full and cropped images. the full image tensor\n",
    "        has the key 'full_img' and the cropped tensor has\n",
    "        the key 'crop_img'\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        A dictionary mapping each task to its logits\n",
    "        \"\"\"\n",
    "        full_img = self.resnet(x)\n",
    "\n",
    "        #full_crop_combined = torch.cat((full_img, crop_img), 1)\n",
    "\n",
    "        #dense_layer_output = self.dense_layers(full_crop_combined)\n",
    "\n",
    "        logit_dict = {}\n",
    "        if hasattr(self, 'pretrained_classifiers'):\n",
    "            for key, classifier in self.pretrained_classifiers.items():\n",
    "                logit_dict[key] = classifier(full_img)\n",
    "        if hasattr(self, 'new_classifiers'):\n",
    "            for key, classifier in self.new_classifiers.items():\n",
    "                logit_dict[key] = classifier(full_img)\n",
    "\n",
    "        return logit_dict\n",
    "\n",
    "    def freeze_core(self):\n",
    "        \"\"\"Freeze all core model layers\"\"\"\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = Fals\n",
    "\n",
    "    def freeze_all_pretrained(self):\n",
    "        \"\"\"Freeze pretrained classifier layers and core model layers\"\"\"\n",
    "        self.freeze_core()\n",
    "        if hasattr(self, 'pretrained_classifiers'):\n",
    "            for param in self.pretrained_classifiers.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            print('There are no pretrained_classifier layers to be frozen.')\n",
    "\n",
    "    def unfreeze_pretrained_classifiers(self):\n",
    "        \"\"\"Unfreeze pretrained classifier layers\"\"\"\n",
    "        if hasattr(self, 'pretrained_classifiers'):\n",
    "            for param in self.pretrained_classifiers.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            print('There are no pretrained_classifier layers to be unfrozen.')\n",
    "\n",
    "    def unfreeze_pretrained_classifiers_and_core(self):\n",
    "        \"\"\"Unfreeze pretrained classifiers and core model layers\"\"\"\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.unfreeze_pretrained_classifiers()\n",
    "\n",
    "    def save(self, folder, model_id):\n",
    "        \"\"\"\n",
    "        Saves the model state dicts to a specific folder.\n",
    "        Each part of the model is saved separately to allow for\n",
    "        new classifiers to be added later.\n",
    "\n",
    "        Note: if the model has `pretrained_classifiers` and `new_classifers`,\n",
    "        they will be combined into the `pretrained_classifiers_dict`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        folder: str or Path\n",
    "            place to store state dictionaries\n",
    "        model_id: int\n",
    "            unique id for this model\n",
    "\n",
    "        Side Effects\n",
    "        ------------\n",
    "        saves two files:\n",
    "            - folder / f'resnet_dict_{model_id}.pth'\n",
    "            - folder / f'pretrained_classifiers_dict_{model_id}.pth'\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'pretrained_classifiers'):\n",
    "            classifiers_to_save = copy.deepcopy(self.new_classifiers)\n",
    "        else:\n",
    "            # Pytorch's update method isn't working because it doesn't think ModuleDict is a Mapping\n",
    "            classifiers_to_save = copy.deepcopy(self.pretrained_classifiers)\n",
    "            if hasattr(self, 'new_classifiers'):\n",
    "                for key, module in self.new_classifiers.items():\n",
    "                    classifiers_to_save[key] = module\n",
    "\n",
    "        folder = Path(folder)\n",
    "        folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        torch.save(\n",
    "            self.resnet.state_dict(),\n",
    "            folder / f'resnet_dict_{model_id}.pth'\n",
    "        )\n",
    "        torch.save(\n",
    "            classifiers_to_save.state_dict(),\n",
    "            folder / f'pretrained_classifiers_dict_{model_id}.pth'\n",
    "        )\n",
    "\n",
    "    def load(self, folder, model_id):\n",
    "        \"\"\"\n",
    "        Loads the model state dicts from a specific folder.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        folder: str or Path\n",
    "            place where state dictionaries are stored\n",
    "        model_id: int\n",
    "            unique id for this model\n",
    "\n",
    "        Side Effects\n",
    "        ------------\n",
    "        loads from two files:\n",
    "            - folder / f'resnet_dict_{model_id}.pth'\n",
    "            - folder / f'pretrained_classifiers_dict_{model_id}.pth'\n",
    "        \"\"\"\n",
    "        #folder = Path(folder)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.resnet.load_state_dict(torch.load(folder +'/'+ f'resnet_dict_{model_id}.pth'))\n",
    "            \n",
    "            self.pretrained_classifiers.load_state_dict(\n",
    "                torch.load(folder / f'pretrained_classifiers_dict_{model_id}.pth')\n",
    "            )\n",
    "        else:\n",
    "            self.resnet.load_state_dict(\n",
    "                torch.load(\n",
    "                    folder / f'resnet_dict_{model_id}.pth',\n",
    "                    map_location=lambda storage,\n",
    "                    loc: storage\n",
    "                )\n",
    "            )\n",
    "        \n",
    "            self.pretrained_classifiers.load_state_dict(\n",
    "                torch.load(\n",
    "                    folder / f'pretrained_classifiers_dict_{model_id}.pth',\n",
    "                    map_location=lambda storage,\n",
    "                    loc: storage\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def export(self, folder, model_id, model_name=None):\n",
    "        \"\"\"\n",
    "        Exports the entire model state dict to a specific folder.\n",
    "        Note: if the model has `pretrained_classifiers` and `new_classifiers`,\n",
    "        they will be combined into the `pretrained_classifiers` attribute before being saved.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        folder: str or Path\n",
    "            place to store state dictionaries\n",
    "        model_id: int\n",
    "            unique id for this model\n",
    "        model_name: str (defaults to None)\n",
    "            Name to store model under, if None, will default to `multi_task_bert_{model_id}.pth`\n",
    "\n",
    "        Side Effects\n",
    "        ------------\n",
    "        saves one file:\n",
    "            - folder / model_name\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'new_classifiers'):\n",
    "            hold_new_classifiers = copy.deepcopy(self.new_classifiers)\n",
    "        else:\n",
    "            hold_new_classifiers = None\n",
    "\n",
    "        hold_pretrained_classifiers = None\n",
    "        if not hasattr(self, 'pretrained_classifiers'):\n",
    "            self.pretrained_classifiers = copy.deepcopy(self.new_classifiers)\n",
    "        else:\n",
    "            hold_pretrained_classifiers = copy.deepcopy(self.pretrained_classifiers)\n",
    "            # Pytorch's update method isn't working because it doesn't think ModuleDict is a Mapping\n",
    "            if hasattr(self, 'new_classifiers'):\n",
    "                for key, module in self.new_classifiers.items():\n",
    "                    self.pretrained_classifiers[key] = module\n",
    "        if hasattr(self, 'new_classifiers'):\n",
    "            del self.new_classifiers\n",
    "\n",
    "        if model_name is None:\n",
    "            model_name = f'multi_task_resnet_{model_id}.pth'\n",
    "\n",
    "        folder = Path(folder)\n",
    "        folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        torch.save(\n",
    "            self.state_dict(),\n",
    "            folder / model_name\n",
    "        )\n",
    "        if hold_pretrained_classifiers is not None:\n",
    "            self.pretrained_classifiers = hold_pretrained_classifiers\n",
    "        else:\n",
    "            del self.pretrained_classifiers\n",
    "\n",
    "        if hold_new_classifiers is not None:\n",
    "            self.new_classifiers = hold_new_classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResnetForMultiTaskClassification(\n",
    "    new_task_dict=new_task_dict,\n",
    "    load_pretrained_resnet=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "lr_last = 1e-2\n",
    "lr_main = 1e-4\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.resnet.parameters(), 'lr': lr_main},\n",
    "    {'params': model.new_classifiers.parameters(), 'lr': lr_last},\n",
    "    \n",
    "])\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size= 4, gamma= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = MultiTaskLearner(model, TrainLoader, ValidLoader, new_task_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>attack_train_loss</th>\n",
       "      <th>attack_val_loss</th>\n",
       "      <th>attack_acc</th>\n",
       "      <th>cards_train_loss</th>\n",
       "      <th>cards_val_loss</th>\n",
       "      <th>cards_acc</th>\n",
       "      <th>turncounter_train_loss</th>\n",
       "      <th>turncounter_val_loss</th>\n",
       "      <th>turncounter_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5.813186</td>\n",
       "      <td>20.480856</td>\n",
       "      <td>3.362564</td>\n",
       "      <td>6.050823</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>9.469371</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.801910</td>\n",
       "      <td>45.452368</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.897026</td>\n",
       "      <td>2.321326</td>\n",
       "      <td>0.795331</td>\n",
       "      <td>5.901299</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.076745</td>\n",
       "      <td>0.207065</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.875185</td>\n",
       "      <td>0.083194</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.384623</td>\n",
       "      <td>0.872330</td>\n",
       "      <td>0.380775</td>\n",
       "      <td>2.295447</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.417688</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.363745</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.269163</td>\n",
       "      <td>0.718666</td>\n",
       "      <td>0.317839</td>\n",
       "      <td>1.887572</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.231201</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.242925</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.152203</td>\n",
       "      <td>0.717088</td>\n",
       "      <td>0.124498</td>\n",
       "      <td>1.885724</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.133552</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.197869</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.101304</td>\n",
       "      <td>0.735356</td>\n",
       "      <td>0.081647</td>\n",
       "      <td>1.934233</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.092086</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.130639</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.100545</td>\n",
       "      <td>0.654007</td>\n",
       "      <td>0.133052</td>\n",
       "      <td>1.718293</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.046010</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.105303</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.196440</td>\n",
       "      <td>0.450437</td>\n",
       "      <td>0.185399</td>\n",
       "      <td>1.178312</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.254179</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.164883</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.091072</td>\n",
       "      <td>0.368469</td>\n",
       "      <td>0.088733</td>\n",
       "      <td>0.968099</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.016666</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.150530</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.148534</td>\n",
       "      <td>0.358295</td>\n",
       "      <td>0.145955</td>\n",
       "      <td>0.939197</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.071307</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.210419</td>\n",
       "      <td>0.003395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 best model saved with loss of 0.3582952976046363\n"
     ]
    }
   ],
   "source": [
    "learn.fit(\n",
    "    num_epochs=10,\n",
    "    loss_function=loss_function,\n",
    "    scheduler=exp_lr_scheduler,\n",
    "    step_scheduler_on_batch=False,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    best_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(folder='models/', model_id='fgo_model1')\n",
    "\n",
    "model.export(folder='models/', model_id='fgo_model1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.7822, -0.0801,  1.0502,  ..., -1.0219, -1.0904, -1.1247],\n",
       "          [-0.3883,  0.1939,  1.1187,  ..., -1.1247, -1.1932, -1.2274],\n",
       "          [ 0.4508,  0.7591,  1.2557,  ..., -1.3473, -1.3987, -1.4158],\n",
       "          ...,\n",
       "          [-1.1760, -1.2103, -1.2617,  ..., -1.7069, -1.5699, -1.4843],\n",
       "          [-1.3130, -1.3473, -1.4158,  ..., -1.5357, -1.3644, -1.2788],\n",
       "          [-1.3815, -1.4158, -1.4843,  ..., -1.4500, -1.2788, -1.1760]],\n",
       "\n",
       "         [[-0.4776,  0.2752,  1.4307,  ..., -1.1429, -1.2129, -1.2479],\n",
       "          [-0.0749,  0.5553,  1.5007,  ..., -1.2479, -1.3179, -1.3529],\n",
       "          [ 0.7829,  1.1331,  1.6758,  ..., -1.4755, -1.5280, -1.5455],\n",
       "          ...,\n",
       "          [-1.6856, -1.7031, -1.7381,  ..., -1.8081, -1.6681, -1.5805],\n",
       "          [-1.7731, -1.7906, -1.8431,  ..., -1.6331, -1.4580, -1.3704],\n",
       "          [-1.8081, -1.8431, -1.8957,  ..., -1.5455, -1.3704, -1.2654]],\n",
       "\n",
       "         [[-0.8110, -0.0790,  1.0714,  ..., -1.0201, -1.0898, -1.1247],\n",
       "          [-0.4101,  0.1825,  1.1411,  ..., -1.1247, -1.1944, -1.2293],\n",
       "          [ 0.4265,  0.7402,  1.2805,  ..., -1.3513, -1.4036, -1.4210],\n",
       "          ...,\n",
       "          [-1.5953, -1.6302, -1.6476,  ..., -1.6476, -1.5081, -1.4210],\n",
       "          [-1.6824, -1.7173, -1.7522,  ..., -1.4733, -1.2990, -1.2119],\n",
       "          [-1.7173, -1.7522, -1.8044,  ..., -1.3861, -1.2119, -1.1073]]]],\n",
       "       device='cuda:0', grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models as torch_models\n",
    "\n",
    "from tonks.vision.helpers import _dense_block, _Identity\n",
    "\n",
    "\n",
    "class ResnetForMultiTaskClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch image attribute model. This model allows you to load\n",
    "    in some pretrained tasks in addition to creating new ones.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    To instantiate a completely new instance of ResnetForMultiTaskClassification\n",
    "    and load the weights into this architecture you can set `pretrained` to True\n",
    "\n",
    "    ```\n",
    "        model = ResnetForMultiTaskClassification(\n",
    "            new_task_dict=new_task_dict,\n",
    "            load_pretrained_resnet = True\n",
    "        )\n",
    "\n",
    "        DO SOME TRAINING\n",
    "\n",
    "        model.save(SOME_FOLDER, SOME_MODEL_ID)\n",
    "    ```\n",
    "\n",
    "    To instantiate an instance of ResnetForMultiTaskClassification that has layers for\n",
    "    pretrained tasks and new tasks, you would do the following:\n",
    "    ```\n",
    "        model = ResnetForMultiTaskClassification(\n",
    "            pretrained_task_dict=pretrained_task_dict,\n",
    "            new_task_dict=new_task_dict\n",
    "        )\n",
    "\n",
    "        model.load(SOME_FOLDER, SOME_MODEL_DICT)\n",
    "\n",
    "        DO SOME TRAINING\n",
    "    ```\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pretrained_task_dict: dict\n",
    "        dictionary mapping each pretrained task to the number of labels it has\n",
    "    new_task_dict: dict\n",
    "        dictionary mapping each new task to the number of labels it has\n",
    "    load_pretrained_resnet: boolean\n",
    "        flag for whether or not to load in pretrained weights for ResNet50.\n",
    "        useful for the first round of training before there are fine tuned weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_task_dict=None, new_task_dict=None, load_pretrained_resnet=False):\n",
    "        super(ResnetForMultiTaskClassification, self).__init__()\n",
    "\n",
    "        self.resnet = torch_models.resnet50(pretrained=load_pretrained_resnet)\n",
    "        self.resnet.fc = _Identity()\n",
    "\n",
    "        if pretrained_task_dict is not None:\n",
    "            pretrained_layers = {}\n",
    "            for key, task_size in pretrained_task_dict.items():\n",
    "                pretrained_layers[key] = nn.Linear(2048, task_size)\n",
    "            self.pretrained_classifiers = nn.ModuleDict(pretrained_layers)\n",
    "        if new_task_dict is not None:\n",
    "            new_layers = {}\n",
    "            for key, task_size in new_task_dict.items():\n",
    "                new_layers[key] = nn.Linear(2048, task_size)\n",
    "            self.new_classifiers = nn.ModuleDict(new_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines forward pass for image model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: dict of image tensors containing tensors for\n",
    "        full and cropped images. the full image tensor\n",
    "        has the key 'full_img' and the cropped tensor has\n",
    "        the key 'crop_img'\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        A dictionary mapping each task to its logits\n",
    "        \"\"\"\n",
    "        full_img = self.resnet(x)\n",
    "\n",
    "        #full_crop_combined = torch.cat((full_img, crop_img), 1)\n",
    "\n",
    "        #dense_layer_output = self.dense_layers(full_crop_combined)\n",
    "\n",
    "        logit_dict = {}\n",
    "        if hasattr(self, 'pretrained_classifiers'):\n",
    "            for key, classifier in self.pretrained_classifiers.items():\n",
    "                logit_dict[key] = classifier(full_img)\n",
    "        if hasattr(self, 'new_classifiers'):\n",
    "            for key, classifier in self.new_classifiers.items():\n",
    "                logit_dict[key] = classifier(full_img)\n",
    "\n",
    "        return logit_dict\n",
    "\n",
    "    \n",
    "\n",
    "imsize = 224\n",
    "\n",
    "loader = transforms.Compose([transforms.Resize((imsize,imsize)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "scale=transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor()])\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def image_loader(image_name):\n",
    "    \"\"\"load image, returns cuda tensor\"\"\"\n",
    "    image = loader(image_name).float()\n",
    "    image = Variable(image, requires_grad=True)\n",
    "    image = image.unsqueeze(0)  #this is for VGG, may not be needed for ResNet\n",
    "    return image.cuda()  #assumes that you're using GPU\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "img = Image.open('fgo_image_datasets/attack/attack/a1.JPG')\n",
    "img = image_loader(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_task_dict\n",
    "model2 = ResnetForMultiTaskClassification(pretrained_task_dict=new_task_dict,\n",
    "    load_pretrained_resnet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResnetForMultiTaskClassification(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "    (fc): _Identity()\n",
       "  )\n",
       "  (pretrained_classifiers): ModuleDict(\n",
       "    (attack): Linear(in_features=2048, out_features=2, bias=True)\n",
       "    (cards): Linear(in_features=2048, out_features=3, bias=True)\n",
       "    (turncounter): Linear(in_features=2048, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "model2.load_state_dict(torch.load(\"models/multi_task_resnet_fgo_model1.pth\"))\n",
    "model2 = model2.to(device)\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not_attack'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(1)\n",
    "\n",
    "\n",
    "def get_preds(image,category):\n",
    "    if category == 'attack':\n",
    "        labels = { 0:'attack', 1:'not_attack'}\n",
    "    if category == 'cards':\n",
    "        labels = { 0:'arts', 1:'buster',2:'quick'}\n",
    "    if category == 'turncounter':\n",
    "        labels = {0:'one', 1:'three',2: 'two'}\n",
    "    full_preds = model2(image)\n",
    "    preds_out = softmax(full_preds[category])\n",
    "    label_out = labels[preds_out.cpu().data.numpy().argmax()]\n",
    "    \n",
    "    return label_out\n",
    "\n",
    "\n",
    "get_preds(img,'attack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not_attack'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('fgo_image_datasets/attack/not_attack/a1.JPG')\n",
    "img = image_loader(img)\n",
    "get_preds(img,'attack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attack'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('fgo_image_datasets/attack/attack/a1.JPG')\n",
    "img = image_loader(img)\n",
    "get_preds(img,'attack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buster'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#buster\n",
    "img = Image.open('fgo_image_datasets/cards/buster/b1.JPG')\n",
    "img = image_loader(img)\n",
    "get_preds(img,'cards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'three'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('fgo_image_datasets/turncounter/three/1.JPG')\n",
    "img = image_loader(img)\n",
    "get_preds(img,'turncounter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('fgo_image_datasets/turncounter/one/1.JPG')\n",
    "img = image_loader(img)\n",
    "get_preds(img,'turncounter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
